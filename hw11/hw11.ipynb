{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda16a2c",
   "metadata": {},
   "source": [
    "##### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a49350",
   "metadata": {},
   "source": [
    "(5pts) Consider the translation dataset provided for Activity 21. Develop a sequence-to-sequence\n",
    "Transformer network to translate from English to French and translate the sentence: “I am going\n",
    "to do well on the next test.” Show your work, by trying different variants of the model, measuring\n",
    "their performance, and discussing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file from https://download.pytorch.org/tutorial/data.zip\n",
      "Extracting contents to .\n",
      "Extraction completed.\n",
      "\n",
      "First 10 lines of the file:\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "def download_and_unzip(url, extract_to='.'):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from the given URL and unzips it to the given directory.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to download the ZIP file from.\n",
    "    extract_to (str): The directory to extract the contents of the ZIP file to.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        print(f\"Downloading file from {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract all the contents of the zip file in the directory 'extract_to'\n",
    "        with ZipFile(BytesIO(response.content)) as zip_file:\n",
    "            print(f\"Extracting contents to {extract_to}\")\n",
    "            zip_file.extractall(path=extract_to)\n",
    "            print(\"Extraction completed.\")\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")  # Python 3.6\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "\n",
    "download_and_unzip('https://download.pytorch.org/tutorial/data.zip', '.')\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode2ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def preprocess_string(s):\n",
    "    s = unicode2ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "ENG_PREFIXES = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# Filter pairs\n",
    "def filter_pairs(pairs):\n",
    "    subset = []\n",
    "    for fr, en in pairs:\n",
    "        if len(fr.split(' ')) > MAX_LENGTH:\n",
    "            continue\n",
    "        if len(en.split(' ')) > MAX_LENGTH:\n",
    "            continue\n",
    "        if not en.startswith(ENG_PREFIXES):\n",
    "            continue\n",
    "        subset.append((fr, en))\n",
    "    return subset\n",
    "\n",
    "\n",
    "# Read the data\n",
    "def read_dataset(lang1, lang2, reverse=False):\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    print(\"Processing lines...\")\n",
    "    pairs = [[preprocess_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "\n",
    "    # Filter pairs by length and content\n",
    "    pairs = filter_pairs(pairs)\n",
    "\n",
    "    print(\"Finished processing\")\n",
    "    return pairs\n",
    "\n",
    "corpus_pairs = read_dataset('eng', 'fra', reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(corpus_pairs)} translation pairs.\")\n",
    "print(\"Here are 10 examples\")\n",
    "for _ in range(10):\n",
    "    fr, en = random.choice(corpus_pairs)\n",
    "    print(f\"French: {fr} -> English: {en}\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def tokenize(self, sentence, seq_len=None):\n",
    "        # Add Start Of Sentence token\n",
    "        token_seq_idx = [self.word2index[\"SOS\"]]\n",
    "\n",
    "        # Tokenize each word in sentence\n",
    "        for tkn in sentence.split():\n",
    "            token_seq_idx.append(self.word2index[tkn if tkn in self.word2index else \"UNK\"])\n",
    "\n",
    "        # Add End Of Sentence token\n",
    "        token_seq_idx.append(self.word2index[\"EOS\"])\n",
    "\n",
    "        if seq_len is not None:\n",
    "            if len(token_seq_idx) < seq_len:\n",
    "                # Pad to desired lengh\n",
    "                token_seq_idx += [self.word2index[\"PAD\"]] * (seq_len - len(token_seq_idx))\n",
    "            else:\n",
    "                # Trim sentence to length\n",
    "                token_seq_idx = token_seq_idx[:seq_len]\n",
    "\n",
    "        return token_seq_idx\n",
    "\n",
    "    def list2sentence(self, seq_ids):\n",
    "        return \" \".join([self.index2word[idx] for idx in seq_ids])\n",
    "    def build_vocab(self, max_words=3000):\n",
    "      # Sort words by frequency\n",
    "      sorted_words = sorted(self.word2count.items(), key=lambda x: x[1], reverse=True)\n",
    "      sorted_words = sorted_words[:max_words - 4]  # Account for PAD, SOS, EOS, UNK\n",
    "\n",
    "      # Reset mappings except special tokens\n",
    "      self.word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "      self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "      self.n_words = 4\n",
    "\n",
    "      for word, _ in sorted_words:\n",
    "          self.word2index[word] = self.n_words\n",
    "          self.index2word[self.n_words] = word\n",
    "          self.n_words += 1\n",
    "\n",
    "print(\"Creating French and English dictionaries.\")\n",
    "fr_vocab = Lang('fr')\n",
    "en_vocab = Lang('en')\n",
    "\n",
    "# First pass: count words\n",
    "for fr, en in corpus_pairs:\n",
    "    fr_vocab.addSentence(fr)\n",
    "    en_vocab.addSentence(en)\n",
    "\n",
    "# Limit vocabulary\n",
    "fr_vocab.build_vocab(max_words=3000)\n",
    "en_vocab.build_vocab(max_words=3000)\n",
    "\n",
    "print(f\"French: {fr_vocab.n_words} words found.\")\n",
    "print(f\"English: {en_vocab.n_words} words found.\")\n",
    "\n",
    "def create_dataloaders(batch_size):\n",
    "    # Create two huge tensor with all english and french sentences\n",
    "    n = len(corpus_pairs)\n",
    "    french_seqs_ids = torch.zeros((n, MAX_LENGTH+2)).long()\n",
    "    english_seqs_ids = torch.zeros((n, MAX_LENGTH+2)).long()\n",
    "\n",
    "    for idx, (fr, en) in enumerate(corpus_pairs):\n",
    "        french_seqs_ids[idx] = torch.tensor(fr_vocab.tokenize(fr, seq_len=MAX_LENGTH+2))\n",
    "        english_seqs_ids[idx] = torch.tensor(en_vocab.tokenize(en, seq_len=MAX_LENGTH+2))\n",
    "\n",
    "    # Split into training and testing\n",
    "    train_sample_mask = torch.rand((n,)) > 0.3\n",
    "    train_french_seqs_ids = french_seqs_ids[train_sample_mask]\n",
    "    train_english_seqs_ids = english_seqs_ids[train_sample_mask]\n",
    "    test_french_seqs_ids = french_seqs_ids[~train_sample_mask]\n",
    "    test_english_seqs_ids = english_seqs_ids[~train_sample_mask]\n",
    "\n",
    "    # Create train dataloader\n",
    "    train_data = TensorDataset(train_french_seqs_ids.to(device), train_english_seqs_ids.to(device))\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_data = TensorDataset(test_french_seqs_ids.to(device), test_english_seqs_ids.to(device))\n",
    "    # test_dataloader = DataLoader(test_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "    return train_dataloader, test_data\n",
    "\n",
    "# Test the dataloader\n",
    "train_dataloader, test_data = create_dataloaders(32)\n",
    "for fr, en in train_dataloader:\n",
    "    print('Batch | fr =', fr.shape, '| en =', en.shape)\n",
    "    print('First sentence in French: ', fr_vocab.list2sentence(fr[0].tolist()))\n",
    "    print('First sentence in English:', en_vocab.list2sentence(en[0].tolist()))\n",
    "    break\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Visualize Position Embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Position Encodings: (Max Position, Embedding Size) =\", pos_encoding.shape)\n",
    "plt.pcolormesh(pos_encoding.T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.show()\n",
    "\n",
    "class WordPosEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        self.pos_encoding = torch.Tensor(positional_encoding(length=2048, depth=d_model)).float().to(device)\n",
    "        self.pos_encoding.requires_grad = False\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        length = x.shape[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= (self.d_model ** 0.5)\n",
    "        x = x + self.pos_encoding[None, :length, :]\n",
    "        return x\n",
    "\n",
    "embed_fr = WordPosEmbedding(vocab_size=fr_vocab.n_words, d_model=512).to(device)\n",
    "embed_en = WordPosEmbedding(vocab_size=en_vocab.n_words, d_model=512).to(device)\n",
    "\n",
    "# Example usage: embed layer receives a batch of sequences of word indexes (ie, a matrix of size BxL where B is batch size and L sequence lenght)\n",
    "en_sentence = 'i am awesome'\n",
    "en_seq = torch.tensor([en_vocab.word2index[w] for w in en_sentence.split()]).unsqueeze(0)\n",
    "print(en_seq.shape)\n",
    "en_tkn_seq = embed_en(en_seq.to(device))\n",
    "print(en_tkn_seq.shape)\n",
    "\n",
    "fr_sentence = 'je plaisante'\n",
    "fr_seq = torch.tensor([fr_vocab.word2index[w] for w in fr_sentence.split()]).unsqueeze(0)\n",
    "print(fr_seq.shape)\n",
    "fr_tkn_seq = embed_fr(fr_seq.to(device))\n",
    "print(fr_tkn_seq.shape)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        return self.norm(x + res)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.add_norm = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        attn_output, attn_scores = self.mha.forward(\n",
    "            query=x, key=context, value=context)\n",
    "        x = self.add_norm(x, attn_output)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "sample_ca = CrossAttention(d_model=512, num_heads=2).to(device)\n",
    "print('Batch of English Sentences:', en_tkn_seq.shape)\n",
    "print('Batch of French Sentences:', fr_tkn_seq.shape)\n",
    "print('Output of Cross-Attention:', sample_ca(en_tkn_seq.to(device), fr_tkn_seq.to(device)).shape)\n",
    "\n",
    "class GlobalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.add_norm = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x)\n",
    "        x = self.add_norm(x, attn_output)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_gsa = GlobalSelfAttention(d_model=512, num_heads=2).to(device)\n",
    "print('Batch of French Sentences:', fr_tkn_seq.shape)\n",
    "print('Output of Global Self-Attention:', sample_gsa(fr_tkn_seq.to(device)).shape)\n",
    "\n",
    "from torch.nn import Transformer as TF\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.add_norm = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        causal_mask = TF.generate_square_subsequent_mask(x.shape[1], device=device)\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            attn_mask=causal_mask)\n",
    "        x = self.add_norm(x, attn_output)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "sample_csa = CausalSelfAttention(d_model=512, num_heads=2).to(device)\n",
    "print('Batch of English Sentences:', en_tkn_seq.shape)\n",
    "print('Output of Causal Self-Attention:', sample_csa(en_tkn_seq.to(device)).shape)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.ffnet = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff*d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_ff*d_model, d_model),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.add_norm = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_norm(x, self.ffnet(x))\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "sample_ffnet = FeedForward(d_model=512, d_ff=4).to(device)\n",
    "print('Batch of English Sentences:', en_tkn_seq.shape)\n",
    "print('Output of Causal Self-Attention:', sample_ffnet(en_tkn_seq.to(device)).shape)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = WordPosEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "\n",
    "# Example usage\n",
    "encoder = Encoder(num_layers=3, d_model=512, num_heads=8, dff=4, vocab_size=fr_vocab.n_words).to(device)\n",
    "print('Batch of English Sentences:', fr_seq.shape)\n",
    "print('Output of Causal Self-Attention:', encoder(fr_seq.to(device)).shape)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = WordPosEmbedding(vocab_size=vocab_size,\n",
    "                                              d_model=d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)])\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "decoder = Decoder(num_layers=3, d_model=512, num_heads=8, dff=4, vocab_size=fr_vocab.n_words).to(device)\n",
    "print('Batch of French Sentences:', fr_seq.shape)\n",
    "print('Batch of English Sentences:', en_seq.shape)\n",
    "fr_feats = encoder(fr_seq.to(device))\n",
    "tgt_feats = decoder(en_seq.to(device), fr_feats)\n",
    "print('Output of Causal Self-Attention:', tgt_feats.shape)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Extracts global representations from the context sequence\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        # Processes the predictions using a causal decoder.\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Predicts the next token using a final linear layer classifier.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "transformer = Transformer(num_layers=3, d_model=512, num_heads=8, dff=4,\n",
    "                          input_vocab_size=fr_vocab.n_words,\n",
    "                          target_vocab_size=en_vocab.n_words).to(device)\n",
    "print('Batch of French Sentences:', fr_seq.shape)\n",
    "print('Batch of English Sentences:', en_seq.shape)\n",
    "print('Output of Causal Self-Attention:', transformer(fr_seq.to(device), en_seq.to(device)).shape)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(transformer, en_sentence):\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        # The French sentence is tokenized and converted to a batch of B=1\n",
    "        english_tkns = torch.tensor(en_vocab.tokenize(en_sentence)).long().unsqueeze(0).to(device)\n",
    "\n",
    "        # First, the sentence to be translated is encoded using the transformer encoder.\n",
    "        english_feats = transformer.encoder(english_tkns)\n",
    "\n",
    "        # The translation sentence is initialized with SOS token\n",
    "        decoded_tkns = torch.tensor([[fr_vocab.word2index['SOS']]]).long().to(device)\n",
    "\n",
    "        # We'll keep track of the predicted logits in order to compute the perplexity\n",
    "        pred_logits = []\n",
    "\n",
    "        # Then, we evaluate the decoder, to generate the next words in the translation, one word at a time.\n",
    "        for i in range(MAX_LENGTH-1):\n",
    "            next_pred_feat = transformer.decoder(decoded_tkns, english_feats)[:, -1]\n",
    "            next_pred_logit = transformer.final_layer(next_pred_feat)\n",
    "            next_pred = next_pred_logit.argmax(dim=1, keepdims=True)\n",
    "            pred_logits.append(next_pred_logit)\n",
    "            if next_pred.item() == fr_vocab.word2index['EOS']:\n",
    "                break\n",
    "            decoded_tkns = torch.cat((decoded_tkns, next_pred), dim=1)\n",
    "\n",
    "        decoded_tkns = decoded_tkns.squeeze(0) # squeeze batch dimension\n",
    "        translation_words = fr_vocab.list2sentence(decoded_tkns[1:].tolist())\n",
    "        pred_logits = torch.cat(pred_logits, 0)\n",
    "    return translation_words, decoded_tkns, pred_logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_one_epoch(transformer, n=100):\n",
    "    transformer.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss, perplexity = 0., 0.\n",
    "    for i in tqdm.tqdm(range(n), desc='[EVAL]'):\n",
    "        fr_tkns, en_tkns = random.choice(test_data)\n",
    "        en = en_vocab.list2sentence(en_tkns[en_tkns > 2].tolist())\n",
    "        _, _, pred_logits = evaluate(transformer, en)\n",
    "        l = criterion(pred_logits, en_tkns[1:1+len(pred_logits)]).item()\n",
    "        loss += l\n",
    "        perplexity += np.exp(l)\n",
    "    return loss / n, perplexity / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_randomly(transformer, n=3):\n",
    "    for i in range(n):\n",
    "        fr_tkn, en_tkn = random.choice(test_data)\n",
    "        en = en_vocab.list2sentence(en_tkn[en_tkn > 2].tolist())\n",
    "        fr = fr_vocab.list2sentence(fr_tkn[fr_tkn > 2].tolist())\n",
    "        print('>', fr)\n",
    "        print('=', en)\n",
    "        output_sentence, _, _ = evaluate(transformer, en)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "translate_randomly(transformer)\n",
    "loss, perplexity = evaluate_one_epoch(transformer)\n",
    "print('Loss = ', loss)\n",
    "print('Perplexity = ', perplexity)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, transformer, optimizer, criterion):\n",
    "    transformer.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for fr_tensor, en_tensor in dataloader:\n",
    "        fr_past = fr_tensor[:, :-1]\n",
    "        fr_target = fr_tensor[:, 1:]\n",
    "        # print(en_past.shape, en_target.shape, fr_tensor.shape)\n",
    "\n",
    "        preds = transformer(fr_past, en_tensor)\n",
    "        # print(preds.shape)\n",
    "\n",
    "        loss = criterion(\n",
    "            preds.flatten(0, 1),\n",
    "            fr_target.flatten(0, 1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, transformer, optimizer, n_epochs,\n",
    "          print_every=5, plot_every=1):\n",
    "    plot_losses = []\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(n_epochs), desc='[TRAIN]'):\n",
    "        loss = train_epoch(train_dataloader, transformer, optimizer, criterion)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            te_loss, te_perplexity = evaluate_one_epoch(transformer)\n",
    "            print(f'[Epoch={epoch}/{n_epochs}] Training Loss={loss:.4f}. Test Loss = {te_loss:.4f}. Test Perplexity = {te_perplexity:.2f}')\n",
    "            translate_randomly(transformer, n=3)\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_losses.append(loss)\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0005\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders(batch_size)\n",
    "\n",
    "transformer = Transformer(num_layers=num_layers, d_model=256, num_heads=8, dff=4,\n",
    "                          input_vocab_size=en_vocab.n_words,\n",
    "                          target_vocab_size=fr_vocab.n_words).to(device)\n",
    "\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "train(train_dataloader, transformer, optimizer, epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56fe7",
   "metadata": {},
   "source": [
    "##### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44994e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
