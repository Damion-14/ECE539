{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5135,
     "status": "ok",
     "timestamp": 1664676580498,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "WSmZaiC5llbI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# for easier reading np\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18790,
     "status": "ok",
     "timestamp": 1664676599283,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "eb8SXW8XlPOl",
    "outputId": "5764c7f7-0c5e-49c1-b1fe-c68f3c178785"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = torch.tensor(iris.data[:, :3], dtype=torch.float32)  # we only take the first three features.\n",
    "y = torch.tensor(iris.data[:, 3], dtype=torch.float32)   # we use the fourth feature as the target.\n",
    "\n",
    "# Uncomment this line for problem 2 (logistic regression)\n",
    "# X = torch.tensor(iris.data, dtype=torch.float32)\n",
    "# y = torch.tensor(iris.target == 2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train torch.Size([120, 3])\n",
      "X_test torch.Size([30, 3])\n"
     ]
    }
   ],
   "source": [
    "# Partition the data into Training and Testing (80:20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=0)\n",
    "\n",
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664676599284,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "MGmYZ7CzdfJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch torch.Size([10, 3]) tensor([7.9000, 3.8000, 6.4000])\n",
      "y_batch torch.Size([10]) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    \n",
    "    # The examples are read at random, in no particular order\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = indices[i:i + batch_size]\n",
    "        yield features[j], labels[j]\n",
    "\n",
    "# Check data reader\n",
    "for X_batch, y_batch in data_iter(batch_size=10, features=X_train, labels=y_train):\n",
    "    print('X_batch', X_batch.shape, X_batch[0])\n",
    "    print('y_batch', y_batch.shape, y_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664676599545,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "8gOc3GnEdyIv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[ 0.0033],\n",
      "        [-0.0109],\n",
      "        [-0.0088]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([[0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Initializing Model Parameters\n",
    "w = torch.nn.Parameter(data=torch.zeros((3, 1)), requires_grad=True)\n",
    "torch.nn.init.normal_(w, mean=0, std=0.01)\n",
    "b = torch.nn.Parameter(data=torch.zeros((1, 1)), requires_grad=True)\n",
    "print('w', w)\n",
    "print('b', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1664676599657,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "GWN7--ZAd3eS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch torch.Size([10, 3]) tensor([5.1000, 2.5000, 3.0000])\n",
      "out_batch torch.Size([10, 1]) tensor([-0.0370], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Model\n",
    "def linreg(X, w, b): \n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return X@w + b\n",
    "\n",
    "# Check model\n",
    "for X_batch, y_batch in data_iter(batch_size=10, features=X_train, labels=y_train):\n",
    "    out_batch = linreg(X_batch, w, b)\n",
    "    print('X_batch', X_batch.shape, X_batch[0])\n",
    "    print('out_batch', out_batch.shape, out_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1664676599658,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "Q3MoZB-0d64m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err = tensor(1.3333)\n"
     ]
    }
   ],
   "source": [
    "# Defining the Loss Function\n",
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return torch.mean((y_hat - y.view(y_hat.shape))**2 / 2)\n",
    "\n",
    "def rsquare(y_hat, y):\n",
    "    mse = torch.sum((y - yhat)**2)\n",
    "    var = torch.sum((y - torch.mean(y_test))**2)\n",
    "    return 1 - mse / var\n",
    "\n",
    "# Check loss\n",
    "err = squared_loss(torch.tensor([1, 2, 3]), torch.tensor([3, 2, 1]))\n",
    "print('err =', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664676599658,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "siaLaTWwd-tT"
   },
   "outputs": [],
   "source": [
    "# Defining the Optimization Algorithm\n",
    "def sgd(params, grads, lr):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    for p, g in zip(params, grads):\n",
    "        p.data -= lr * g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am64FhNQeHjJ"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664676599658,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "v6JmOk7weJ0N"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1664676599782,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "NZXVBu91eLav",
    "outputId": "3b1e0602-f3bd-4a0a-bf52-93a22b3f5b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000, test loss 1.057, train loss 1.350\n",
      "epoch 001, test loss 0.095, train loss 0.108\n",
      "epoch 002, test loss 0.083, train loss 0.075\n",
      "epoch 003, test loss 0.058, train loss 0.055\n",
      "epoch 004, test loss 0.070, train loss 0.054\n",
      "epoch 005, test loss 0.047, train loss 0.037\n",
      "epoch 006, test loss 0.045, train loss 0.033\n",
      "epoch 007, test loss 0.038, train loss 0.028\n",
      "epoch 008, test loss 0.034, train loss 0.026\n",
      "epoch 009, test loss 0.033, train loss 0.025\n",
      "epoch 010, test loss 0.034, train loss 0.023\n",
      "epoch 011, test loss 0.032, train loss 0.024\n",
      "epoch 012, test loss 0.038, train loss 0.024\n",
      "epoch 013, test loss 0.040, train loss 0.025\n",
      "epoch 014, test loss 0.034, train loss 0.022\n",
      "epoch 015, test loss 0.033, train loss 0.022\n",
      "epoch 016, test loss 0.033, train loss 0.022\n",
      "epoch 017, test loss 0.033, train loss 0.021\n",
      "epoch 018, test loss 0.035, train loss 0.021\n",
      "epoch 019, test loss 0.042, train loss 0.025\n",
      "epoch 020, test loss 0.033, train loss 0.021\n",
      "epoch 021, test loss 0.034, train loss 0.021\n",
      "epoch 022, test loss 0.035, train loss 0.021\n",
      "epoch 023, test loss 0.033, train loss 0.021\n",
      "epoch 024, test loss 0.035, train loss 0.021\n",
      "epoch 025, test loss 0.033, train loss 0.021\n",
      "epoch 026, test loss 0.034, train loss 0.021\n",
      "epoch 027, test loss 0.032, train loss 0.023\n",
      "epoch 028, test loss 0.032, train loss 0.021\n",
      "epoch 029, test loss 0.038, train loss 0.022\n",
      "epoch 030, test loss 0.031, train loss 0.021\n",
      "epoch 031, test loss 0.033, train loss 0.020\n",
      "epoch 032, test loss 0.033, train loss 0.020\n",
      "epoch 033, test loss 0.033, train loss 0.020\n",
      "epoch 034, test loss 0.031, train loss 0.021\n",
      "epoch 035, test loss 0.032, train loss 0.020\n",
      "epoch 036, test loss 0.033, train loss 0.020\n",
      "epoch 037, test loss 0.032, train loss 0.020\n",
      "epoch 038, test loss 0.031, train loss 0.021\n",
      "epoch 039, test loss 0.035, train loss 0.021\n",
      "epoch 040, test loss 0.035, train loss 0.021\n",
      "epoch 041, test loss 0.031, train loss 0.020\n",
      "epoch 042, test loss 0.033, train loss 0.020\n",
      "epoch 043, test loss 0.031, train loss 0.020\n",
      "epoch 044, test loss 0.030, train loss 0.022\n",
      "epoch 045, test loss 0.034, train loss 0.020\n",
      "epoch 046, test loss 0.031, train loss 0.020\n",
      "epoch 047, test loss 0.030, train loss 0.021\n",
      "epoch 048, test loss 0.032, train loss 0.019\n",
      "epoch 049, test loss 0.033, train loss 0.020\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parameters of the model\n",
    "torch.nn.init.normal_(w, mean=0, std=0.01)\n",
    "torch.nn.init.zeros_(b)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Evaluate model\n",
    "    with torch.no_grad():\n",
    "        yhat = net(X_test, w, b)[:, 0]\n",
    "        test_Rsq = rsquare(yhat, y_test)\n",
    "        test_l = loss(yhat, y_test)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yhat = net(X_train, w, b)[:, 0]\n",
    "        train_Rsq = rsquare(yhat, y_train)\n",
    "        train_l = loss(yhat, y_train)\n",
    "    \n",
    "    print(f'epoch {epoch:03d}, test loss {float(test_l):.3f}, train loss {float(train_l):.3f}') \n",
    "\n",
    "    # Train for one epoch\n",
    "    for X_batch, y_batch in data_iter(batch_size=10, features=X_train, labels=y_train):\n",
    "        # Use model to compute predictions\n",
    "        yhat = net(X_batch, w, b)\n",
    "        l = loss(yhat, y_batch)  # Minibatch loss in `X_batch` and `y_batch`\n",
    "\n",
    "        # Compute gradients by back propagation\n",
    "        l.backward()\n",
    "\n",
    "        # Update parameters using their gradient\n",
    "        sgd([w, b], [w.grad, b.grad], lr)\n",
    "\n",
    "        # Reset gradients\n",
    "        w.grad = b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1664676718080,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 300
    },
    "id": "jPOy0pgTeNT8",
    "outputId": "f172cb26-325c-491d-f08d-938f63b1b9f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept =  [[-0.038]]\n",
      "Coefficients = \n",
      " [[-0.041]\n",
      " [-0.044]\n",
      " [ 0.426]]\n",
      "Train R square =  0.938\n",
      "Test R square =  0.873\n"
     ]
    }
   ],
   "source": [
    "# R2 = 1 - MSE/var(y)\n",
    "print('Intercept = ', b.detach().numpy())\n",
    "print('Coefficients = \\n', w.detach().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat = net(X_train, w, b)[:, 0]\n",
    "    R_sq_train = rsquare(yhat, y_train)\n",
    "    \n",
    "    yhat = net(X_test, w, b)[:, 0]\n",
    "    R_sq_test = rsquare(yhat, y_test)\n",
    "    \n",
    "print('Train R square = ', format(R_sq_train.numpy(),\".3f\"))\n",
    "print('Test R square = ', format(R_sq_test.numpy(),\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
