{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P1) Analyse the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time Traveller (for so it will be convenient to speak of him)\n",
      "was expounding a recondite matter to us. His grey eyes shone and\n",
      "twinkled, and his usually pale face was flushed and animated. The\n",
      "fire burned brightly, and the soft radiance of the incandescent\n",
      "lights in the lilies of silver caught the bubbles that flashed and\n",
      "passed in our glasses. Our chairs, being his patents, embraced and\n",
      "caressed us rather than submitted to be sat upon, and there was that\n",
      "luxurious after-dinner atmosphere when thought roams gracefully\n",
      "free of the trammels of precision. And he put it to us in this\n",
      "way--marking the points with a lean forefinger--as we sat and lazily\n"
     ]
    }
   ],
   "source": [
    "corpus = [line.strip() for line in open('./TheTimeMachine.txt') if line.strip()][2:]\n",
    "print(\"\\n\".join(corpus[:10]))\n",
    "\n",
    "# Tokenize the sentences into words. All lower case. Ignore punctuation.\n",
    "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
    "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
    "corpus = [word for line in corpus for word in line.split()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Found 3001 unique words in the provided corpus (of size 32767).\n",
      "  * Created vocabulary from corpus.\n",
      "  * The 10 most common words are the following:\n",
      "[('the', 2260), ('i', 1266), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 3000\n",
    "tkn_counter = Counter([word for word in corpus])\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
    "vocab[\"/UNK\"] = len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random snippet from the corpus.\n",
      "  * Token IDS:\t tensor([ 312,   54,   27,   42,  600,    3, 1472,  110,   15,  108,  439,    3,\n",
      "          18,  108,   72,  130,    4,  849,   51,   52,  370,  187,    3, 1472,\n",
      "        2275,  231,  182,    0,  235,   17,    4, 1473,   64,   37,  371,  151,\n",
      "         130,    0,  849,    7,   20, 2276,   26,  188,  219,   63,  140, 1462,\n",
      "           7,    4])\n",
      "  * Words:\t\t course we have no means of staying back for any length of time any more than a savage or an animal has of staying six feet above the ground but a civilized man is better off than the savage in this respect he can go up against gravitation in a\n"
     ]
    }
   ],
   "source": [
    "class TextCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, snippet_len=50):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.snippet_len = snippet_len\n",
    "\n",
    "        # Vocabulary (word-to-index mapping)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Inverse vocabulary (index-to-word mapping)\n",
    "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "\n",
    "    def convert2idx(self, word_sequence):\n",
    "        return [self.vocab[word if word in self.vocab else \"/UNK\"] for word in word_sequence]\n",
    "\n",
    "    def convert2words(self, idx_sequence):\n",
    "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.snippet_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        snippet = self.corpus[idx:idx+self.snippet_len]\n",
    "        snippet = torch.tensor(self.convert2idx(snippet))\n",
    "        return snippet\n",
    "\n",
    "# Test dataset function\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=50)\n",
    "snippet = dataset[1234]\n",
    "print(\"\\nRandom snippet from the corpus.\")\n",
    "print(\"  * Token IDS:\\t\", snippet)\n",
    "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in snippet.tolist()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P2) The CBOW Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 6.4501\n",
      "Epoch [2/100], Loss: 4.4778\n",
      "Epoch [3/100], Loss: 3.5596\n",
      "Epoch [4/100], Loss: 2.9592\n",
      "Epoch [5/100], Loss: 2.5170\n",
      "Epoch [6/100], Loss: 2.1958\n",
      "Epoch [7/100], Loss: 1.9744\n",
      "Epoch [8/100], Loss: 1.8043\n",
      "Epoch [9/100], Loss: 1.6863\n",
      "Epoch [10/100], Loss: 1.5872\n",
      "Epoch [11/100], Loss: 1.5175\n",
      "Epoch [12/100], Loss: 1.4666\n",
      "Epoch [13/100], Loss: 1.4177\n",
      "Epoch [14/100], Loss: 1.3788\n",
      "Epoch [15/100], Loss: 1.3430\n",
      "Epoch [16/100], Loss: 1.3044\n",
      "Epoch [17/100], Loss: 1.2911\n",
      "Epoch [18/100], Loss: 1.2571\n",
      "Epoch [19/100], Loss: 1.2416\n",
      "Epoch [20/100], Loss: 1.2216\n",
      "Epoch [21/100], Loss: 1.2056\n",
      "Epoch [22/100], Loss: 1.1903\n",
      "Epoch [23/100], Loss: 1.1848\n",
      "Epoch [24/100], Loss: 1.1647\n",
      "Epoch [25/100], Loss: 1.1616\n",
      "Epoch [26/100], Loss: 1.1508\n",
      "Epoch [27/100], Loss: 1.1452\n",
      "Epoch [28/100], Loss: 1.1376\n",
      "Epoch [29/100], Loss: 1.1248\n",
      "Epoch [30/100], Loss: 1.1130\n",
      "Epoch [31/100], Loss: 1.1051\n",
      "Epoch [32/100], Loss: 1.1079\n",
      "Epoch [33/100], Loss: 1.0963\n",
      "Epoch [34/100], Loss: 1.0889\n",
      "Epoch [35/100], Loss: 1.0907\n",
      "Epoch [36/100], Loss: 1.0712\n",
      "Epoch [37/100], Loss: 1.0782\n",
      "Epoch [38/100], Loss: 1.0799\n",
      "Epoch [39/100], Loss: 1.0672\n",
      "Epoch [40/100], Loss: 1.0647\n",
      "Epoch [41/100], Loss: 1.0582\n",
      "Epoch [42/100], Loss: 1.0599\n",
      "Epoch [43/100], Loss: 1.0469\n",
      "Epoch [44/100], Loss: 1.0435\n",
      "Epoch [45/100], Loss: 1.0446\n",
      "Epoch [46/100], Loss: 1.0387\n",
      "Epoch [47/100], Loss: 1.0546\n",
      "Epoch [48/100], Loss: 1.0505\n",
      "Epoch [49/100], Loss: 1.0417\n",
      "Epoch [50/100], Loss: 1.0411\n",
      "Epoch [51/100], Loss: 1.0203\n",
      "Epoch [52/100], Loss: 1.0219\n",
      "Epoch [53/100], Loss: 1.0192\n",
      "Epoch [54/100], Loss: 1.0242\n",
      "Epoch [55/100], Loss: 1.0243\n",
      "Epoch [56/100], Loss: 1.0234\n",
      "Epoch [57/100], Loss: 1.0179\n",
      "Epoch [58/100], Loss: 1.0114\n",
      "Epoch [59/100], Loss: 1.0158\n",
      "Epoch [60/100], Loss: 1.0212\n",
      "Epoch [61/100], Loss: 1.0143\n",
      "Epoch [62/100], Loss: 1.0142\n",
      "Epoch [63/100], Loss: 1.0172\n",
      "Epoch [64/100], Loss: 1.0088\n",
      "Epoch [65/100], Loss: 0.9937\n",
      "Epoch [66/100], Loss: 0.9939\n",
      "Epoch [67/100], Loss: 1.0037\n",
      "Epoch [68/100], Loss: 0.9965\n",
      "Epoch [69/100], Loss: 0.9893\n",
      "Epoch [70/100], Loss: 0.9949\n",
      "Epoch [71/100], Loss: 0.9963\n",
      "Epoch [72/100], Loss: 0.9983\n",
      "Epoch [73/100], Loss: 0.9941\n",
      "Epoch [74/100], Loss: 0.9951\n",
      "Epoch [75/100], Loss: 0.9926\n",
      "Epoch [76/100], Loss: 0.9831\n",
      "Epoch [77/100], Loss: 0.9964\n",
      "Epoch [78/100], Loss: 0.9880\n",
      "Epoch [79/100], Loss: 0.9849\n",
      "Epoch [80/100], Loss: 0.9853\n",
      "Epoch [81/100], Loss: 0.9796\n",
      "Epoch [82/100], Loss: 0.9818\n",
      "Epoch [83/100], Loss: 0.9901\n",
      "Epoch [84/100], Loss: 0.9791\n",
      "Epoch [85/100], Loss: 0.9761\n",
      "Epoch [86/100], Loss: 0.9617\n",
      "Epoch [87/100], Loss: 0.9853\n",
      "Epoch [88/100], Loss: 0.9930\n",
      "Epoch [89/100], Loss: 0.9805\n",
      "Epoch [90/100], Loss: 0.9795\n",
      "Epoch [91/100], Loss: 0.9764\n",
      "Epoch [92/100], Loss: 0.9673\n",
      "Epoch [93/100], Loss: 0.9763\n",
      "Epoch [94/100], Loss: 0.9660\n",
      "Epoch [95/100], Loss: 0.9688\n",
      "Epoch [96/100], Loss: 0.9807\n",
      "Epoch [97/100], Loss: 0.9787\n",
      "Epoch [98/100], Loss: 0.9675\n",
      "Epoch [99/100], Loss: 0.9719\n",
      "Epoch [100/100], Loss: 0.9529\n"
     ]
    }
   ],
   "source": [
    "# Define the Word2Vec CBOW model\n",
    "class Word2Vec_CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(???)\n",
    "        self.linear = nn.Linear(???)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_embeds = ???\n",
    "        avg_context_embeds = ???\n",
    "        logits = ???\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters\n",
    "context_len = 2\n",
    "vocab_size = len(dataset.vocab)\n",
    "embedding_dim = 128\n",
    "learning_rate = 5e-3\n",
    "num_epochs = 100\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=2*context_len + 1)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create and train the CBOW model\n",
    "w2v = Word2Vec_CBOW(vocab_size, embedding_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(w2v.parameters(), lr=learning_rate)\n",
    "context_idx = [idx for idx in range(2*context_len+1) if idx != context_len]\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for snippet in train_loader:\n",
    "        context = snippet[:, context_idx].to(device)\n",
    "        target = snippet[:, context_len].to(device)\n",
    "        logits = w2v(context)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-db85c7ead957>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Extract the word embeddings to analyze it\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mword_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mw2v\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the word embeddings to analyze it\n",
    "word_embeddings = w2v.embeddings.weight.detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## P3) Next-word prediction using CBOW embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class NextWordPredictionMLP(nn.Module):\n",
    "    def __init__(self, num_context, embedding, depth=3, hidden_dim=50):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for d in range(depth):\n",
    "            if d == 0:\n",
    "                in_chans = num_context * embedding.embedding_dim\n",
    "                out_chans = hidden_dim\n",
    "            elif d == depth - 1:\n",
    "                in_chans = hidden_dim\n",
    "                out_chans = embedding.num_embeddings\n",
    "            else:\n",
    "                in_chans = out_chans = hidden_dim\n",
    "\n",
    "            self.mlp.add_module(f'linear{d}', nn.Linear(in_chans, out_chans))\n",
    "            self.mlp.add_module(f'bn{d}', nn.BatchNorm1d(out_chans))\n",
    "            self.mlp.add_module(f'act{d}', nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, context):\n",
    "        emb = self.embedding(context).flatten(1)\n",
    "        return self.mlp(emb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep000] | Loss 7.657 \t Perplexity  2115.721\n",
      "[Ep001] | Loss 6.586 \t Perplexity  724.957\n",
      "[Ep002] | Loss 5.910 \t Perplexity  368.606\n",
      "[Ep003] | Loss 5.414 \t Perplexity  224.628\n",
      "[Ep004] | Loss 5.046 \t Perplexity  155.433\n",
      "[Ep005] | Loss 4.758 \t Perplexity  116.530\n",
      "[Ep006] | Loss 4.528 \t Perplexity  92.540\n",
      "[Ep007] | Loss 4.347 \t Perplexity  77.284\n",
      "[Ep008] | Loss 4.194 \t Perplexity  66.294\n",
      "[Ep009] | Loss 4.054 \t Perplexity  57.600\n",
      "[Ep010] | Loss 3.937 \t Perplexity  51.280\n",
      "[Ep011] | Loss 3.828 \t Perplexity  45.967\n",
      "[Ep012] | Loss 3.729 \t Perplexity  41.624\n",
      "[Ep013] | Loss 3.646 \t Perplexity  38.327\n",
      "[Ep014] | Loss 3.564 \t Perplexity  35.312\n",
      "[Ep015] | Loss 3.475 \t Perplexity  32.310\n",
      "[Ep016] | Loss 3.407 \t Perplexity  30.170\n",
      "[Ep017] | Loss 3.348 \t Perplexity  28.432\n",
      "[Ep018] | Loss 3.271 \t Perplexity  26.338\n",
      "[Ep019] | Loss 3.220 \t Perplexity  25.038\n",
      "[Ep020] | Loss 3.162 \t Perplexity  23.609\n",
      "[Ep021] | Loss 3.104 \t Perplexity  22.288\n",
      "[Ep022] | Loss 3.044 \t Perplexity  20.990\n",
      "[Ep023] | Loss 2.999 \t Perplexity  20.057\n",
      "[Ep024] | Loss 2.945 \t Perplexity  19.018\n",
      "[Ep025] | Loss 2.901 \t Perplexity  18.186\n",
      "[Ep026] | Loss 2.847 \t Perplexity  17.232\n",
      "[Ep027] | Loss 2.804 \t Perplexity  16.515\n",
      "[Ep028] | Loss 2.767 \t Perplexity  15.904\n",
      "[Ep029] | Loss 2.723 \t Perplexity  15.228\n",
      "[Ep030] | Loss 2.679 \t Perplexity  14.570\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loss_fcn, optimizer, dataloader):\n",
    "    total_loss = 0.\n",
    "    for it, batch in enumerate(dataloader):\n",
    "        batch_past = batch[:, :T].to(device)\n",
    "        batch_now = batch[:, -1].to(device)\n",
    "\n",
    "        pred_now = model(batch_past)\n",
    "        l = loss_fcn(pred_now, batch_now)\n",
    "        total_loss += l.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss = total_loss / len(dataloader)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def fit(model, loss_fcn, dataloader, optimizer, epochs=30):\n",
    "    \"\"\"\n",
    "    Helper function to train a model.\n",
    "    \"\"\"\n",
    "    for ep in range(epochs):\n",
    "        loss = train_one_epoch(model, loss_fcn, optimizer, dataloader)\n",
    "        print(f\"[Ep{ep:03}] | Loss {loss:.3f} \\t Perplexity  {np.exp(loss):.3f}\")\n",
    "\n",
    "\n",
    "T = 10\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=T+1)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "model = NextWordPredictionMLP(T, w2v.embeddings, depth=2, hidden_dim=50).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fcn = F.cross_entropy\n",
    "\n",
    "fit(model, loss_fcn, dataloader, opt, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prompt = \" \".join(corpus[:10])\n",
    "    print(\"PROMPT:\", prompt)\n",
    "    context = torch.tensor([dataset.vocab[word] for word in prompt.split()]).to(device)\n",
    "    context = context.unsqueeze(0)  # Reshape it into a batch of 1\n",
    "    model.train(False)\n",
    "    for _ in range(100):\n",
    "        next_word_logits = model(context)\n",
    "        next_word_idx = next_word_logits[:, :-1].argmax(dim=1)\n",
    "        next_word = dataset.inv_vocab[next_word_idx[0].item()]\n",
    "        context = torch.cat((context[:, 1:], next_word_idx.unsqueeze(1)), 1)\n",
    "        print(next_word, end=' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}