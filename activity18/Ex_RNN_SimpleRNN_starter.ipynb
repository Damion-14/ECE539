{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time Machine, by H. G. Wells [1898]\n",
      "I\n",
      "The Time Traveller (for so it will be convenient to speak of him)\n",
      "was expounding a recondite matter to us. His grey eyes shone and\n",
      "twinkled, and his usually pale face was flushed and animated. The\n",
      "fire burned brightly, and the soft radiance of the incandescent\n",
      "lights in the lilies of silver caught the bubbles that flashed and\n",
      "passed in our glasses. Our chairs, being his patents, embraced and\n",
      "caressed us rather than submitted to be sat upon, and there was that\n",
      "luxurious after-dinner atmosphere when thought roams gracefully\n"
     ]
    }
   ],
   "source": [
    "corpus = [line.strip() for line in open('TheTimeMachine.txt') if line.strip()]\n",
    "print(\"\\n\".join(corpus[:10]))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "# All lower caps. Ignore punctuation.\n",
    "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
    "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
    "corpus = [word for line in corpus for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Found 4582 unique words in the provided corpus (of size 32776).\n",
      "  * Created vocabulary from corpus.\n",
      "  * The 10 most common words are the following:\n",
      "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "tkn_counter = Counter([word for word in corpus])\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
    "vocab[\"/UNK\"] = len(vocab)\n",
    "print(f\"  * Found {len(vocab)} unique words in the provided corpus (of size {len(corpus)}).\\n\"\n",
    "      f\"  * Created vocabulary from corpus.\\n\"\n",
    "      f\"  * The 10 most common words are the following:\")\n",
    "print(tkn_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "\n",
      "Random sequence from the corpus.\n",
      "  * Token IDS:\t tensor([  21,    5, 2211,  682,  275, 1430,  235,   15,   10,   21,  114,  196,\n",
      "         830,   13,  180,   13,    1,  502,   29,   21,   21,  150,    3,  312,\n",
      "           8,    4, 1047,  330,    4,  330,    3,  683, 2212,  187,   42,  400,\n",
      "         591,   28, 1427,   21,    8, 1431,  187,    4, 1047, 2213,   58,  132,\n",
      "          90,  244])\n",
      "  * Words:\t\t you to accept anything without reasonable ground for it you will soon admit as much as i need from you you know of course that a mathematical line a line of thickness nil has no real existence they taught you that neither has a mathematical plane these things are mere\n"
     ]
    }
   ],
   "source": [
    "class TextCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, sequence_len=50):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "        # Vocabulary (word-to-index mapping)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Inverse vocabulary (index-to-word mapping)\n",
    "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "\n",
    "    def convert2idx(self, word_sequence):\n",
    "        return [self.vocab[word if word in self.vocab else \"/UNK\"] for word in word_sequence]\n",
    "\n",
    "    def convert2words(self, idx_sequence):\n",
    "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.corpus) - self.sequence_len) // self.sequence_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx * self.sequence_len\n",
    "        snippet = self.corpus[idx:idx+self.sequence_len]\n",
    "        snippet = torch.tensor(self.convert2idx(snippet))\n",
    "        return snippet\n",
    "\n",
    "# Test dataset function\n",
    "dataset = TextCorpusDataset(corpus, vocab, sequence_len=50)\n",
    "sequence = dataset[4]\n",
    "print(len(dataset))\n",
    "print(\"\\nRandom sequence from the corpus.\")\n",
    "print(\"  * Token IDS:\\t\", sequence)\n",
    "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in sequence.tolist()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.hidden_dim = vocab_size, hidden_dim\n",
    "\n",
    "        self.inp2state = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.state2state = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.state2out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def initial_state(self, batch_size, device):\n",
    "        return torch.zeros((batch_size, self.hidden_dim)).to(device)\n",
    "\n",
    "    def forward(self, inp_seq, state=None):\n",
    "        n_steps, batch_size = inp_seq.shape[:2]\n",
    "\n",
    "        # If state is not provided, get initial state.\n",
    "        if state is None:\n",
    "            state = self.initial_state(batch_size, inp_seq.device)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(n_steps):\n",
    "            inp_at_t = inp_seq[t]\n",
    "            # Compute new state: ht = tanh(Wx2h * xt + Wh2h * ht-1 + bh)\n",
    "            state = torch.tanh(self.inp2state(inp_at_t) + self.state2state(state)) # state is h_t-1\n",
    "            # Compute output: ot = Wh2o * ht + bo\n",
    "            out = self.state2out(state)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs, 0)\n",
    "\n",
    "        return outputs, state\n",
    "\n",
    "hidden_dim = 256\n",
    "model = SimpleRNN(len(vocab), hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4582])\n",
      "torch.Size([5, 1, 4582])\n",
      "['fix', 'nature', 'collapsed', 'fix', 'endowed']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"today is too darn cold\".split()\n",
    "inp = F.one_hot(torch.tensor(dataset.convert2idx(sentence), device=device), len(vocab)).float() # Must have size 5, 1, 4582  (Note that in RNNs the batch is often the 2nd dimension, not the first)\n",
    "inp = inp.unsqueeze(1)\n",
    "print(inp.shape)\n",
    "Yhat, _ = model(inp)\n",
    "print(Yhat.shape)\n",
    "predicted_indices = torch.argmax(Yhat, dim=-1).squeeze(1)\n",
    "Yhat_words = dataset.convert2words(predicted_indices.tolist())\n",
    "print(Yhat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not mean to ask you to accept anything tampering palaeontology expression declaration concerned hastings servants hull explain impartiality'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prefix, num_preds, model, vocab):\n",
    "    \"\"\"Generates a sentence following the `prefix`.\"\"\"\n",
    "    prefix = torch.tensor(dataset.convert2idx(prefix.split()), device=device).long()\n",
    "\n",
    "    state, outputs = None, [prefix[0]]\n",
    "    for i in range(1, len(prefix) + num_preds):\n",
    "        # Prepare the current token to feed the model\n",
    "        inp = F.one_hot(outputs[-1], len(vocab)).float()\n",
    "        inp = inp[None, None]\n",
    "\n",
    "        # Compute the prediction of the next token\n",
    "        yhat, state = model(inp, state)\n",
    "\n",
    "        if i < len(prefix):\n",
    "            # During warmup (while parsing the prefix), we ignore the model prediction\n",
    "            outputs.append(prefix[i])\n",
    "        else:\n",
    "            # Otherwise, append the model prediction to the list\n",
    "            yhat = yhat[..., :-1].argmax(dim=-1).reshape(1).long()\n",
    "            outputs.append(yhat)\n",
    "    return ' '.join([dataset.inv_vocab[tkn.item()] for tkn in outputs])\n",
    "\n",
    "generate('i do not mean to ask you to accept anything', 10, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Perplexity 7226.5. Loss: 8.886\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 1 | Perplexity 7024.6. Loss: 8.857\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 2 | Perplexity 6820.5. Loss: 8.828\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 3 | Perplexity 6604.6. Loss: 8.796\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 4 | Perplexity 6354.8. Loss: 8.757\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 5 | Perplexity 6001.8. Loss: 8.700\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 6 | Perplexity 4811.2. Loss: 8.479\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 7 | Perplexity 1761.7. Loss: 7.474\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 8 | Perplexity 1218.4. Loss: 7.105\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 9 | Perplexity 1039.2. Loss: 6.946\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 10 | Perplexity 949.3. Loss: 6.856\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 11 | Perplexity 899.7. Loss: 6.802\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 12 | Perplexity 869.0. Loss: 6.767\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 13 | Perplexity 848.2. Loss: 6.743\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 14 | Perplexity 833.3. Loss: 6.725\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 15 | Perplexity 822.2. Loss: 6.712\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 16 | Perplexity 813.6. Loss: 6.701\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 17 | Perplexity 806.7. Loss: 6.693\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 18 | Perplexity 801.2. Loss: 6.686\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 19 | Perplexity 796.6. Loss: 6.680\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 20 | Perplexity 792.7. Loss: 6.675\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 21 | Perplexity 789.5. Loss: 6.671\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 22 | Perplexity 786.7. Loss: 6.668\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 23 | Perplexity 784.2. Loss: 6.665\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 24 | Perplexity 782.1. Loss: 6.662\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 25 | Perplexity 780.3. Loss: 6.660\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 26 | Perplexity 778.6. Loss: 6.658\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 27 | Perplexity 777.1. Loss: 6.656\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 28 | Perplexity 775.8. Loss: 6.654\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 29 | Perplexity 774.7. Loss: 6.652\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 30 | Perplexity 773.6. Loss: 6.651\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 31 | Perplexity 772.7. Loss: 6.650\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 32 | Perplexity 771.8. Loss: 6.649\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 33 | Perplexity 771.0. Loss: 6.648\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 34 | Perplexity 770.3. Loss: 6.647\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 35 | Perplexity 769.6. Loss: 6.646\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 36 | Perplexity 769.0. Loss: 6.645\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 37 | Perplexity 768.5. Loss: 6.644\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 38 | Perplexity 768.0. Loss: 6.644\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 39 | Perplexity 767.5. Loss: 6.643\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 40 | Perplexity 767.1. Loss: 6.643\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 41 | Perplexity 766.7. Loss: 6.642\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 42 | Perplexity 766.3. Loss: 6.642\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 43 | Perplexity 766.0. Loss: 6.641\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 44 | Perplexity 765.6. Loss: 6.641\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 45 | Perplexity 765.3. Loss: 6.640\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 46 | Perplexity 765.1. Loss: 6.640\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 47 | Perplexity 764.8. Loss: 6.640\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 48 | Perplexity 764.6. Loss: 6.639\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 49 | Perplexity 764.3. Loss: 6.639\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 50 | Perplexity 764.1. Loss: 6.639\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 51 | Perplexity 763.9. Loss: 6.638\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 52 | Perplexity 763.7. Loss: 6.638\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "def train_on_sequence(seq, model, optimizer, unroll=5):\n",
    "    \"\"\"Train the model within a batch of long text sequences.\"\"\"\n",
    "    batch_size, num_tokens = seq.shape\n",
    "\n",
    "    total_loss, state = 0., None\n",
    "    for i in range(0, num_tokens-unroll-1, unroll):\n",
    "        if state is not None:\n",
    "            state.detach_()\n",
    "\n",
    "        # Define the input sequence along which we will unroll the RNN\n",
    "        x_inp = seq[:, i:i+unroll]   # Must be of size T x B\n",
    "        y_trg = seq[:, i+1:i+unroll+1]   # Must be of size T x B\n",
    "\n",
    "        # Forward the model and compute the loss\n",
    "        x_inp = F.one_hot(x_inp, len(vocab)).float()\n",
    "        y_hat, state = model(x_inp, state)\n",
    "        l = loss(y_hat.flatten(0, 1), y_trg.flatten(0, 1).long())\n",
    "        total_loss += l.item()\n",
    "\n",
    "        # Backward step\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    n_batches = (num_tokens-unroll-1) // unroll\n",
    "    return total_loss/n_batches\n",
    "\n",
    "def fit(model, loader, vocab, lr, num_epochs=100, unroll=5):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "    test_prompt = 'i do not mean to ask you to accept anything'\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for sequence in loader:\n",
    "            total_loss += train_on_sequence(sequence.to(device), model, optimizer, unroll=unroll)\n",
    "        total_loss /= len(loader)\n",
    "\n",
    "        print(f'Epoch {epoch} | Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n",
    "        print(generate(test_prompt, 50, model, vocab))\n",
    "\n",
    "num_epochs, lr = 100, 0.001\n",
    "dataset = TextCorpusDataset(corpus, vocab, 100)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "model = SimpleRNN(len(vocab), hidden_dim).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "fit(model, loader, vocab, lr, num_epochs, unroll=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
