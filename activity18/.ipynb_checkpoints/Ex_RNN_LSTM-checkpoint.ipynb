{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4561,
     "status": "ok",
     "timestamp": 1700671563387,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "LrqjZOhN68wT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1700671607377,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "Twc0lhP268wV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4c18abee-0cbc-4890-af26-2aa08ed0e88c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time Machine, by H. G. Wells [1898]\n",
      "I\n",
      "The Time Traveller (for so it will be convenient to speak of him)\n",
      "was expounding a recondite matter to us. His grey eyes shone and\n",
      "twinkled, and his usually pale face was flushed and animated. The\n",
      "fire burned brightly, and the soft radiance of the incandescent\n",
      "lights in the lilies of silver caught the bubbles that flashed and\n",
      "passed in our glasses. Our chairs, being his patents, embraced and\n",
      "caressed us rather than submitted to be sat upon, and there was that\n",
      "luxurious after-dinner atmosphere when thought roams gracefully\n"
     ]
    }
   ],
   "source": [
    "corpus = [line.strip() for line in open('TheTimeMachine.txt') if line.strip()]\n",
    "print(\"\\n\".join(corpus[:10]))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
    "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
    "corpus = [word for line in corpus for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700671609064,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "9JjHJ1KQ68wW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "76d9f10b-1d54-432a-9731-46f0ae14b1a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  * Found 4582 unique words in the provided corpus (of size 32776).\n",
      "  * Created vocabulary from corpus.\n",
      "  * The 10 most common words are the following:\n",
      "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "tkn_counter = Counter([word for word in corpus])\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
    "vocab[\"/UNK\"] = len(vocab)\n",
    "print(f\"  * Found {len(vocab)} unique words in the provided corpus (of size {len(corpus)}).\\n\"\n",
    "      f\"  * Created vocabulary from corpus.\\n\"\n",
    "      f\"  * The 10 most common words are the following:\")\n",
    "print(tkn_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1700671611475,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "5AVXv-6_68wW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "214c7f9d-b47e-4b44-886f-e5ba57ed4af4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random snippet from the corpus.\n",
      "  * Token IDS:\t tensor([ 171,   50,    1,   52,    0,   49, 1176,   36,  133,   13,    1,  377,\n",
      "          14,    4,  506,  697,   85,   18,   20,  855, 2619,    1,    6,   36,\n",
      "           5,  585, 2620,    6, 1632,   59,    4, 1168,   85,    0, 2621,    3,\n",
      "        2622, 2623,   17,    5,  149,    5,    4,  513, 2624,    0, 2625,    3,\n",
      "          82, 1633])\n",
      "  * Words:\t\t space which i or the machine occupied so long as i travelled at a high velocity through time this scarcely mattered i was so to speak attenuated was slipping like a vapour through the interstices of intervening substances but to come to a stop involved the jamming of myself molecule\n"
     ]
    }
   ],
   "source": [
    "class TextCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab, snippet_len=50):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.snippet_len = snippet_len\n",
    "\n",
    "        # Vocabulary (word-to-index mapping)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Inverse vocabulary (index-to-word mapping)\n",
    "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "\n",
    "    def convert2idx(self, word_sequence):\n",
    "        return [self.vocab[word if word in self.vocab else \"/UNK\"] for word in word_sequence]\n",
    "\n",
    "    def convert2words(self, idx_sequence):\n",
    "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.corpus) - self.snippet_len) // self.snippet_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx * self.snippet_len\n",
    "        snippet = self.corpus[idx:idx+self.snippet_len]\n",
    "        snippet = torch.tensor(self.convert2idx(snippet))\n",
    "        return snippet\n",
    "\n",
    "# Test dataset function\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=50)\n",
    "snippet = dataset[123]\n",
    "print(\"\\nRandom snippet from the corpus.\")\n",
    "print(\"  * Token IDS:\\t\", snippet)\n",
    "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in snippet.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9345,
     "status": "ok",
     "timestamp": 1700671621204,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "UKS1qmT368wX",
    "outputId": "a9e93449-9330-49df-ceb6-11de46656757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['silently', 'anyhow', 'silently', 'silently', 'orders']\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=None):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM Parameters\n",
    "        self.input_gate = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.forget_gate = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.candidate = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(input_size+hidden_size, hidden_size)\n",
    "\n",
    "        self.predictor = nn.Linear(hidden_size, input_size) if output_size is not None else nn.Identity()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "\n",
    "    def init_state_cell(self, batch_size, device):\n",
    "        state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "        cell = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "        return state, cell\n",
    "\n",
    "    def forward(self, x, state=None, cell=None):\n",
    "        # Get sequence length and batch size\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        # Initialize hidden and cell states if not provided\n",
    "        if state is None or cell is None:\n",
    "            state, cell = self.init_state_cell(batch_size, x.device)\n",
    "\n",
    "        # Lists to store outputs and cell states for each time step\n",
    "        outputs = []\n",
    "\n",
    "        # Iterate through the sequence\n",
    "        for t in range(seq_len):\n",
    "            # Input at time step t\n",
    "            xh_t = torch.cat((x[t], state), 1)\n",
    "\n",
    "            # Input gate\n",
    "            inp_t = torch.sigmoid(self.input_gate(xh_t))\n",
    "\n",
    "            # Forget gate\n",
    "            forget_t = torch.sigmoid(self.forget_gate(xh_t))\n",
    "\n",
    "            # Cell state\n",
    "            c_tilda_t = torch.tanh(self.candidate(xh_t))\n",
    "            # Mistake: cell = forget_t * cell + (1-forget_t) * c_tilda_t\n",
    "            cell = forget_t * cell + inp_t * c_tilda_t\n",
    "\n",
    "            # Output gate\n",
    "            ot = torch.sigmoid(self.output(xh_t))\n",
    "\n",
    "            # Hidden state update\n",
    "            # Mistake: state = torch.tanh(cell)\n",
    "            state = ot * torch.tanh(cell)\n",
    "\n",
    "            # Normally an LSTM simply outputs the hidden state.\n",
    "            # However, here we want our outputs to be the logits for the predicted next word.\n",
    "            output = self.predictor(state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Stack outputs and cell states along the sequence dimension\n",
    "        outputs = torch.stack(outputs, dim=0)\n",
    "        return outputs, (state, cell)\n",
    "\n",
    "hidden_dim, vocab_size = 256, len(dataset.vocab)\n",
    "model = LSTM(vocab_size, hidden_dim, vocab_size).to(device)\n",
    "\n",
    "sentence = \"today is too darn cold\".split()\n",
    "inp = torch.tensor(dataset.convert2idx(sentence), device=device)[:, None]\n",
    "inp = F.one_hot(inp, len(vocab)).float()\n",
    "Yhat, new_state = model(inp)\n",
    "Yhat = Yhat.squeeze(1).argmax(-1)\n",
    "print(dataset.convert2words(Yhat.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1700671621204,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "jTCDVkd368wX",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5a10f776-60b1-43c1-b02c-6e430cb2b59d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'i do not mean to ask you to accept anything orders orders immediately silently bright silently which immediately bright silently'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prefix, num_preds, model, vocab):\n",
    "    \"\"\"Generates a sentence following the `prefix`.\"\"\"\n",
    "    prefix = torch.tensor(dataset.convert2idx(prefix.split()), device=device).long()\n",
    "\n",
    "    state, cell, outputs = None, None, [prefix[0]]\n",
    "    for i in range(1, len(prefix) + num_preds):\n",
    "        # Prepare one token at a time to feed the model\n",
    "        inp = F.one_hot(outputs[-1], len(vocab)).float()\n",
    "        inp = inp[None, None]\n",
    "\n",
    "        # Compute the prediction for the next token\n",
    "        yhat, (state, cell) = model(inp, state, cell)\n",
    "\n",
    "        if i < len(prefix):\n",
    "            # During warmup (while parsing the prefix), we ignore the model prediction\n",
    "            outputs.append(prefix[i])\n",
    "        else:\n",
    "            # Otherwise, append the model prediction to the output list\n",
    "            yhat = yhat.argmax(dim=-1)[0, 0].long()\n",
    "            outputs.append(yhat)\n",
    "    return ' '.join([dataset.inv_vocab[tkn.item()] for tkn in outputs])\n",
    "\n",
    "generate('i do not mean to ask you to accept anything', 10, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 12872,
     "status": "ok",
     "timestamp": 1700671887329,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "LGf-zJVx68wX",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "3ae2a518-f3cd-4814-a5c2-18aca608c79c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Perplexity 999.6. Loss: 6.907\n",
      "i do not mean to ask you to accept anything the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 1 | Perplexity 612.6. Loss: 6.418\n",
      "i do not mean to ask you to accept anything the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i of the i\n",
      "Epoch 2 | Perplexity 527.5. Loss: 6.268\n",
      "i do not mean to ask you to accept anything i i i i i i i the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the gallery of the\n",
      "Epoch 3 | Perplexity 455.7. Loss: 6.122\n",
      "i do not mean to ask you to accept anything i was the gallery of the gallery of the gallery of the time of the time of the time of the time of of the little of of the morlocks of the morlocks of the gallery of the gallery of of the gallery of of the gallery of of the\n",
      "Epoch 4 | Perplexity 389.7. Loss: 5.965\n",
      "i do not mean to ask you to accept anything i had the gallery of the gallery of the gallery of the gallery of the time of the time of the time of the time of the time of the time of the time of the morlocks of the morlocks of the morlocks of the time of of the time\n",
      "Epoch 5 | Perplexity 333.2. Loss: 5.809\n",
      "i do not mean to ask you to accept anything i had was the time of the time of of the little machine of the time of the time of the time of the time of the time of the time of the time of the time of the morlocks of the morlocks of the time of of the time\n",
      "Epoch 6 | Perplexity 287.0. Loss: 5.660\n",
      "i do not mean to ask you to accept anything i was the time of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the time machine of the\n",
      "Epoch 7 | Perplexity 247.9. Loss: 5.513\n",
      "i do not mean to ask you to accept anything i had have to the time machine of the time machine of the time traveller of the time traveller of the time traveller of the time traveller of the time traveller of the time machine of the time machine of the time machine of the time machine of the little\n",
      "Epoch 8 | Perplexity 214.6. Loss: 5.369\n",
      "i do not mean to ask you to accept anything i had entered the gallery of the time machine of the time machine of the time traveller of the time traveller of the time traveller of the time traveller of the time traveller of the time machine of the time machine of the time machine of the time machine of\n",
      "Epoch 9 | Perplexity 184.3. Loss: 5.216\n",
      "i do not mean to ask you to accept anything i had a gallery of the time machine of the time machine of the psychologist man was the psychologist of the time traveller had had been been young and of the time machine of the time machine of the time machine of the time machine of the time machine of\n"
     ]
    }
   ],
   "source": [
    "def train_on_sequence(seq, model, optimizer, unroll=5):\n",
    "    \"\"\"Train the model within a batch of long text sequences.\"\"\"\n",
    "    batch_size, num_tokens = seq.shape\n",
    "\n",
    "    total_loss, state, cell = 0., None, None\n",
    "    for i in range(0, num_tokens-unroll-1, unroll):\n",
    "        if state is not None:\n",
    "            state.detach_(), cell.detach_()\n",
    "\n",
    "        # Define the input sequence along which we will unroll the RNN\n",
    "        x = seq[:, i:i+unroll].T\n",
    "        y = seq[:, i+1:i+unroll+1].T\n",
    "\n",
    "        # Forward the model and compute the loss\n",
    "        x = F.one_hot(x, len(vocab)).float()\n",
    "        y_hat, (state, cell) = model(x, state, cell)\n",
    "        l = loss(y_hat.flatten(0, 1), y.flatten(0, 1).long())\n",
    "        total_loss += l.item()\n",
    "\n",
    "        # Backward step (clip gradients to prevent exploding gradients)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    n_batches = (num_tokens-unroll-1) // unroll\n",
    "    return total_loss/n_batches\n",
    "\n",
    "def fit(model, loader, vocab, lr, num_epochs=100, unroll=5):\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr)\n",
    "    test_prompt = 'i do not mean to ask you to accept anything'\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for sequence in loader:\n",
    "            total_loss += train_on_sequence(sequence.to(device), model, optimizer, unroll=unroll)\n",
    "        total_loss /= len(loader)\n",
    "\n",
    "        print(f'Epoch {epoch} | Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n",
    "        print(generate(test_prompt, 50, model, vocab))\n",
    "\n",
    "num_epochs, lr = 10, 0.001\n",
    "dataset = TextCorpusDataset(corpus[:25000], vocab, 100)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "model = LSTM(len(dataset.vocab), hidden_dim, output_size=len(dataset.vocab)).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "fit(model, loader, dataset.vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1700671899203,
     "user": {
      "displayName": "Jones Lin",
      "userId": "13641637329812821989"
     },
     "user_tz": 360
    },
    "id": "uKjHxAWH68wY",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "079c6511-3652-433a-b280-bb2fb5d52a23",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 1359.0. Loss: 7.215\n"
     ]
    }
   ],
   "source": [
    "def eval_on_sequence(seq, model, unroll=5):\n",
    "    \"\"\"Train the model within a batch of long text sequences.\"\"\"\n",
    "    batch_size, num_tokens = seq.shape\n",
    "\n",
    "    total_loss, state, cell = 0., None, None\n",
    "    for i in range(0, num_tokens-unroll-1, unroll):\n",
    "        if state is not None:\n",
    "            state.detach_(), cell.detach_()\n",
    "\n",
    "        # Define the input sequence along which we will unroll the RNN\n",
    "        x = seq[:, i:i+unroll].T\n",
    "        y = seq[:, i+1:i+unroll+1].T\n",
    "\n",
    "        # Forward the model and compute the loss\n",
    "        x = F.one_hot(x, len(vocab)).float()\n",
    "        y_hat, (state, cell) = model(x, state, cell)\n",
    "        l = loss(y_hat.flatten(0, 1), y.flatten(0, 1).long())\n",
    "        total_loss += l.item()\n",
    "\n",
    "\n",
    "    n_batches = (num_tokens-unroll-1) // unroll\n",
    "    return total_loss/n_batches\n",
    "\n",
    "def eval(model, loader, vocab, lr, num_epochs=100, unroll=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for sequence in loader:\n",
    "            total_loss += eval_on_sequence(sequence.to(device), model, unroll=unroll)\n",
    "        total_loss /= len(loader)\n",
    "\n",
    "        print(f'Perplexity {np.exp(total_loss):.1f}. Loss: {total_loss:.3f}')\n",
    "\n",
    "num_epochs, lr = 100, 0.001\n",
    "test_dataset = TextCorpusDataset(corpus[25000:], vocab, 100)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "eval(model, test_loader, dataset.vocab, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "Lxik3lM768wY",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "CBHe_OwG68wY",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "S2vTCs2J68wY",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "FrOyzqj768wY",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
