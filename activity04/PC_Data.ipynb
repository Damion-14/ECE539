{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vfwQ6YEhluba"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "import numpy as np\n",
    "\n",
    "# makes printing more human-friendly\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Load data \n",
    "\n",
    "colab = False  # Set to True if using colab\n",
    "if colab:\n",
    "    # May require changing paths to file\n",
    "    drive.mount('/content/drive')\n",
    "    with open('/content/drive/Ex_PC_data.csv', 'r') as f: \n",
    "      data = np.genfromtxt(f,delimiter=',')\n",
    "else:\n",
    "    # May require changing paths to file\n",
    "    with open('Ex_PC_data.csv', 'r') as f: \n",
    "      data = np.genfromtxt(f,delimiter=',')\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Ga08Oc2ZnwqK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 59, 2.0: 71, 3.0: 48}\n",
      "num of samples: 178\n",
      "num of feature dimensions: 13\n",
      "num of classes: 3\n",
      "class 1.0 has 59 samples\n",
      "class 2.0 has 71 samples\n",
      "class 3.0 has 48 samples\n"
     ]
    }
   ],
   "source": [
    "# b) number of samples, features dimension, the number of classes\n",
    "\n",
    "num_samples = len(y)\n",
    "num_feats = len(X[0])\n",
    "num_classes = len(np.unique(y))\n",
    "num_samples_per_class = {value: np.sum(y == value) for value in np.unique(y)}\n",
    "print(num_samples_per_class)\n",
    "\n",
    "print(f'num of samples: {num_samples}')\n",
    "print(f'num of feature dimensions: {num_feats}')\n",
    "print(f'num of classes: {num_classes}')\n",
    "for cls in num_samples_per_class:\n",
    "  print(f'class {cls} has {num_samples_per_class[cls]} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NZpvsgo2oXIb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of NaN before imputation: 6\n",
      "Total of NaN after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# c) check nan, data imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "if np.sum(np.isnan(X)):\n",
    "  print('Total of NaN before imputation:', np.sum(np.isnan(X)))\n",
    "  X = KNNImputer(n_neighbors=2, weights=\"uniform\").fit_transform(X)\n",
    "  print('Total of NaN after imputation:', np.sum(np.isnan(X)))\n",
    "else:\n",
    "  print('no NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q) How are the missing values completed when using KNNImputer?\n",
    "A) It averages the feature from the nearest n neighbors and weighs them based on the weights variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6Ue36ENusGef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  142\n",
      "testing data size:  36\n"
     ]
    }
   ],
   "source": [
    "# d) partition 80/20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training data size: ', X_train.shape[0])\n",
    "print('testing data size: ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TnkQ2QfWvO44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min training data in each dimension, after standardization: [-5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5. -5.]\n",
      "max training data in each dimension, after standardization: [5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "min testing data in each dimension, after standardization: [-4.    -5.305 -3.182 -2.732 -4.13  -3.724 -4.662 -4.245 -5.032 -5.409\n",
      " -4.268 -4.89  -4.63 ]\n",
      "max testing data in each dimension, after standardization: [3.605 2.556 3.021 2.423 0.761 3.    2.574 4.434 1.551 2.247 1.748 4.341\n",
      " 6.048]\n"
     ]
    }
   ],
   "source": [
    "# e) standardize to -5 to 5\n",
    "\n",
    "X_train_min = np.min(X_train, axis=0)\n",
    "X_train_max = np.max(X_train, axis=0)\n",
    "\n",
    "X_train_standardized = ((X_train - X_train_min) / (X_train_max - X_train_min)) * 10 - 5\n",
    "print('min training data in each dimension, after standardization:', np.min(X_train_standardized, axis=0))\n",
    "print('max training data in each dimension, after standardization:', np.max(X_train_standardized, axis=0))\n",
    "\n",
    "# Warning: When standardizing the test set, we should use statistics like min or max computed from the training set.\n",
    "X_test_standardized = ((X_test - X_train_min) / (X_train_max - X_train_min)) * 10 - 5\n",
    "print('min testing data in each dimension, after standardization:', np.min(X_test_standardized, axis=0))\n",
    "print('max testing data in each dimension, after standardization:', np.max(X_test_standardized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training data in each dimension, after standardization: [ 0. -0.  0.  0. -0.  0.  0. -0.  0.  0.  0.  0. -0.]\n",
      "std training data in each dimension, after standardization: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "min testing data in each dimension, after standardization: [ 0.133 -0.161  0.101  0.033 -0.238  0.047  0.127 -0.232 -0.153  0.001\n",
      "  0.023  0.13   0.197]\n",
      "max testing data in each dimension, after standardization: [0.948 0.87  0.91  0.825 0.846 0.907 0.977 0.823 0.9   0.977 0.881 0.91\n",
      " 1.182]\n"
     ]
    }
   ],
   "source": [
    "# f) standardize to 0 mean, unit variance\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train_standardized = (X_train - X_train_mean) / X_train_std\n",
    "print('mean training data in each dimension, after standardization:', np.mean(X_train_standardized, axis=0))\n",
    "print('std training data in each dimension, after standardization:', np.std(X_train_standardized, axis=0))\n",
    "\n",
    "# Warning: When standardizing the test set, we should use statistics like min or max computed from the training set.\n",
    "X_test_standardized = (X_test - X_train_mean) / X_train_std\n",
    "print('min testing data in each dimension, after standardization:', np.mean(X_test_standardized, axis=0))\n",
    "print('max testing data in each dimension, after standardization:', np.std(X_test_standardized, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "MhMyvX7TxO5S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training class distribution:  94\n",
      "validation class distribution:  48\n",
      "training class distribution:  95\n",
      "validation class distribution:  47\n",
      "training class distribution:  95\n",
      "validation class distribution:  47\n"
     ]
    }
   ],
   "source": [
    "# g) k fold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Count the class distributions in each partition\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "  print('training class distribution: ', np.count_nonzero(y_train[train_index].astype(int)))\n",
    "  print('validation class distribution: ', np.count_nonzero(y_train[val_index].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "EqtiN5sX2ER7"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    drive.flush_and_unmount()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
